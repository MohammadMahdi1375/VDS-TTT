# ðŸš€ Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection

The code of our paper "Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection"

## Overview
Learning to adapt pretrained language models to unlabeled, outâ€‘ofâ€‘distribution data is a critical challenge, as models often falter on structurally novel reasoning tasks even while excelling within their training distribution. We introduce a new framework called VDS-TTT - Verifierâ€‘Driven Sample Selection for Testâ€‘Time Training to efficiently address this. We use a learned verifier to score a pool of generated responses and select only from high ranking pseudoâ€‘labeled examples for fine-tuned adaptation. Specifically, for each input query our LLM generates $N$ candidate answers; the verifier assigns a reliability score to each, and the response with the highest confidence and above a fixed threshold is paired with its query for testâ€‘time training. We fineâ€‘tune only lowâ€‘rank LoRA adapter parameters, ensuring adaptation efficiency and fast convergence. Our proposed self-supervised framework is the first to synthesize verifier driven test-time training data for continuous self-improvement of the model. Experiments across three diverse benchmarks and three stateâ€‘ofâ€‘theâ€‘art LLMs demonstrate that VDSâ€‘TTT yields up to a 32.29% relative improvement over the base model and a 6.66% gain compared to verifier-based methods without testâ€‘time training, highlighting its effectiveness and efficiency for onâ€‘theâ€‘fly large language model adaptation.

## Results
